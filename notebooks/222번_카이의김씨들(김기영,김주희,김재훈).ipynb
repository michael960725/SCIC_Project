{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "222번.카이의김씨들(김기영,김주희,김재훈).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qGxU_ZS8DEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7349a2-9198-4c16-a77c-ea27884823c6"
      },
      "source": [
        "\"\"\"commented out for test execution\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YEZSFwyP5fQ"
      },
      "source": [
        "#공용함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUkWw1txP4XA"
      },
      "source": [
        "# 수정금지: 타임스탬프용 함수\n",
        "from datetime import datetime\n",
        "def printt(*args,**kwargs):\n",
        "  now = datetime.now()\n",
        "  now_str = \"{:02}:{:02}:{:02}\".format(now.hour,now.minute,now.second)\n",
        "  print(now_str, *args,**kwargs)\n",
        "  return int(now.hour)*60*60+int(now.minute)*60+int(now.second)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-h1MVpOSRVl"
      },
      "source": [
        "#연관 패키지 설치 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHw8KJbRC4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fc2c7d7-d3f0-4c8d-c3ea-a13362d5eb4f"
      },
      "source": [
        "#TODO: 해당 블럭에 패키지 설치하세요.\n",
        "!pip install attrdict\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 61.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=ca2705febfe8c016a83901cbe6fe5378ecfbd19748d4ac49ffa391def5e54e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress) (1.19.5)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 72.5 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIMuPDLIo0rb"
      },
      "source": [
        "# 파일로딩 (다른작업 금지)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4zJTn7o0LO"
      },
      "source": [
        "#TODO: 해당 블럭에 필요 파일 로딩 코드 넣으시오.\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        ")\n",
        "train_file = '/content/drive/MyDrive/v1/train.txt'\n",
        "test_file = '/content/drive/MyDrive/v1/evaluation.txt'\n",
        "pretrain_bin = '/content/drive/MyDrive/pretrained_model/koelectra_pretrained.bin'\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\n",
        "    '/content/drive/MyDrive/pretrained_model/', do_lower_case = False)\n",
        "config_1 = ElectraConfig.from_pretrained('/content/drive/MyDrive/pretrained_model',\n",
        "                                       num_labels=1)\n",
        "config_2 = ElectraConfig.from_pretrained('/content/drive/MyDrive/pretrained_model',\n",
        "                                       num_labels=16)\n",
        "lines_dict = {}\n",
        "import torch\n",
        "\n",
        "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        lines.append(line.strip())\n",
        "    lines_dict['train'] = lines\n",
        "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        lines.append(line.strip())\n",
        "    lines_dict['test'] = lines"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVIEOA0Swkd"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOOoJjT8Sx3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fad6887-c2e2-4414-fdea-9a4777732a0d"
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_start_time = printt(\"Model building: Start\")\n",
        "_model_build_start_time"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10:09:00 Model building: Start\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36540"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnQUVccSa-V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "547d6e5f-1ed7-4347-d806-e6e1ab755ba4"
      },
      "source": [
        "#TODO: 블럭에 모델 학습 - 빌딩 코드를 넣으세요. (시간측정 구간)\n",
        "\n",
        "# set config args for classification\n",
        "from transformers import (\n",
        "    ElectraConfig,\n",
        "    ElectraTokenizer,\n",
        "    ElectraForSequenceClassification,\n",
        ")\n",
        "import argparse\n",
        "import json\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict\n",
        "\n",
        "lines_dict['input'] = []\n",
        "args = AttrDict(\n",
        "    {\n",
        "        'version': 1,\n",
        "        'data_dir': '',\n",
        "        'train_file': lines_dict['train'],\n",
        "        'test_file': lines_dict['test'],\n",
        "        # 'val_file': 'test.txt',\n",
        "        'input_file': lines_dict['input'],\n",
        "        'task': 'classification',\n",
        "        'config_1': config_1,\n",
        "        'config_2': config_2,\n",
        "        'tokenizer': ElectraTokenizer,\n",
        "        'model': ElectraForSequenceClassification,\n",
        "        'device': 'cuda',\n",
        "        'do_lower_case': False,  \n",
        "        'max_seq_len': 64, \n",
        "        'num_train_epochs': 10, \n",
        "        'weight_decay_1': 0.3, \n",
        "        'weight_decay_2': 0.2,\n",
        "        'gradient_accumulation_steps': 1, \n",
        "        'adam_epsilon': 1e-08, \n",
        "        'warmup_proportion_1': 0.0, \n",
        "        'warmup_proportion_2': 0.0,\n",
        "        'max_steps': -1, \n",
        "        'max_grad_norm': 1.0, \n",
        "        'no_cuda': False, \n",
        "        'tokenizer': tokenizer,\n",
        "        'model_path': '/content/drive/MyDrive/pretrained_model/',\n",
        "        'output_dir_1': 'checkpoints_sentiment', \n",
        "        'output_dir_2': 'checkpoints_classification', \n",
        "        'seed': 42, \n",
        "        'train_batch_size': 16, \n",
        "        'eval_batch_size': 16, \n",
        "        'learning_rate_1': 5e-06,\n",
        "        'learning_rate_2': 5e-05,\n",
        "     }\n",
        ")\n",
        "\n",
        "# make Input Example\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, review_token, label1, label2):\n",
        "        self.review_token = review_token\n",
        "        self.label1 = label1\n",
        "        self.label2 = label2\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "        \n",
        "# make Input Feature\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label1, label2):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label1 = label1\n",
        "        self.label2 = label2\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def load_dataset(args, tokenizer, mode):\n",
        "    features = get_features(args, tokenizer, mode)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor(\n",
        "        [f.input_ids for f in features], \n",
        "        dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(\n",
        "        [f.attention_mask for f in features],\n",
        "         dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(\n",
        "        [f.token_type_ids for f in features],\n",
        "        dtype=torch.long)\n",
        "\n",
        "    all_labels_1 = torch.tensor([f.label1 for f in features],\n",
        "                        dtype=torch.float)\n",
        "    all_labels_2 = torch.tensor([f.label2 for f in features],\n",
        "                              dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids,\n",
        "                            all_attention_mask,\n",
        "                            all_token_type_ids,\n",
        "                            all_labels_1,\n",
        "                            all_labels_2)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_features(args, tokenizer, mode):\n",
        "    # load data and labels using Processor\n",
        "    processor = Processor(args)\n",
        "    try:\n",
        "        examples = processor.get_examples(mode)\n",
        "    except ValueError:\n",
        "        print('possible modes: train, val, test')\n",
        "\n",
        "    # if args.task == 'classification':\n",
        "    \n",
        "    labels1 = [float(example.label1) for example in examples]\n",
        "    labels2 = [int(example.label2) for example in examples]\n",
        "\n",
        "\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        [(example.review_token) for example in examples],\n",
        "        max_length=args.max_seq_len,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    features = []\n",
        "    for i in range(len(examples)):\n",
        "        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "        \n",
        "        # For xlm-roberta\n",
        "        inputs[\"token_type_ids\"] = [0] * len(inputs[\"input_ids\"])  \n",
        "        feature = InputFeatures(**inputs, label1=labels1[i], label2=labels2[i])\n",
        "        features.append(feature)\n",
        "        \n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "class Processor(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "\n",
        "    def _read_file(cls, mode):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        return lines_dict[mode]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines[0:]):\n",
        "            line = line.split(\"\\t\")\n",
        "            review_token = line[0]\n",
        "            if not set_type == 'input':\n",
        "                label1 = line[1]\n",
        "                label2 = line[2]\n",
        "            else:\n",
        "                label1 = 0\n",
        "                label2 = 0\n",
        "            examples.append(InputExample(review_token=review_token, label1=label1\n",
        "                                         , label2=label2))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "        elif mode == 'input':\n",
        "            file_to_read = self.args.input_file\n",
        "\n",
        "        return self._create_examples(\n",
        "            self._read_file(mode), mode\n",
        "        )\n",
        "\n",
        "\n",
        "# TODO: acc. calculating logic impl. + figure adding logic impl.\n",
        "        \n",
        "def train(args, datasets):\n",
        "    train_sampler = RandomSampler(datasets['train'])\n",
        "    train_loader = DataLoader(datasets['train'],\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=args.train_batch_size)\n",
        "    val_sampler = RandomSampler(datasets['val'])\n",
        "    val_loader = DataLoader(datasets['val'],\n",
        "                            sampler=val_sampler,\n",
        "                            batch_size=args.eval_batch_size)\n",
        "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "    \n",
        "    t_total = (\n",
        "        len(dataloaders['train']) // args.gradient_accumulation_steps\n",
        "        * args.num_train_epochs\n",
        "    )\n",
        "\n",
        "    # Load models <model_1: sentiment, model_2: classifiation>\n",
        "    config_1 = args.config_1\n",
        "    model_1 = args.model.from_pretrained(pretrain_bin,\n",
        "                                         config=config_1).to(args.device)\n",
        "    config_2 = args.config_2\n",
        "    model_2 = args.model.from_pretrained(pretrain_bin,\n",
        "                                         config=config_2).to(args.device)\n",
        "\n",
        "   \n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters_1 = [\n",
        "        {\n",
        "            'params': [p for n, p in model_1.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': args.weight_decay_1\n",
        "         },\n",
        "        {\n",
        "            'params': [p for n, p in model_1.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "         }\n",
        "    ]\n",
        "    \n",
        "    optimizer_grouped_parameters_2 = [\n",
        "        {\n",
        "            'params': [p for n, p in model_2.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': args.weight_decay_2\n",
        "         },\n",
        "        {\n",
        "            'params': [p for n, p in model_2.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0\n",
        "         }\n",
        "    ]\n",
        "    optimizer_1 = AdamW(optimizer_grouped_parameters_1,\n",
        "                      lr=args.learning_rate_1,\n",
        "                      eps=args.adam_epsilon)\n",
        "    optimizer_2 = AdamW(optimizer_grouped_parameters_2,\n",
        "                      lr=args.learning_rate_2,\n",
        "                      eps=args.adam_epsilon)\n",
        "    scheduler_1 = get_linear_schedule_with_warmup(\n",
        "        optimizer_1,\n",
        "        num_warmup_steps=int(t_total * args.warmup_proportion_1),\n",
        "        num_training_steps=t_total\n",
        "        )\n",
        "    scheduler_2 = get_linear_schedule_with_warmup(\n",
        "        optimizer_2,\n",
        "        num_warmup_steps=int(t_total * args.warmup_proportion_2),\n",
        "        num_training_steps=t_total\n",
        "        )\n",
        "\n",
        "    if os.path.isfile(os.path.join(args.model_path, \"optimizer.pt\")) \\\n",
        "    and os.path.isfile(os.path.join(args.model_path, \"scheduler.pt\")):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer_1.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_path, \"optimizer.pt\"))\n",
        "            )\n",
        "        optimizer_2.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_path, \"optimizer.pt\"))\n",
        "            )\n",
        "        scheduler_1.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_path, \"scheduler.pt\"))\n",
        "            )\n",
        "        scheduler_2.load_state_dict(\n",
        "            torch.load(os.path.join(args.model_path, \"scheduler.pt\"))\n",
        "            )\n",
        "\n",
        "    print(f'training started <model_1: sentiment, model_2: classifiation>')\n",
        "    global_step = 0\n",
        "    top_acc_1 = 0.\n",
        "    top_acc_2 = 0.\n",
        "    model_1.zero_grad()\n",
        "    model_2.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    for epoch in mb:\n",
        "        mb.main_bar.comment = f'training in progress: epoch {epoch+1}'\n",
        "        accuracies_1, accuracies_2, losses = {}, {}, {}\n",
        "        for stage, dataloader in dataloaders.items():\n",
        "            epoch_iterator = progress_bar(dataloader, parent=mb)\n",
        "            num_correct_1, num_correct_2, num_items = 0, 0, 0\n",
        "            running_loss = 0\n",
        "            if stage == 'train':\n",
        "                model_1.train()\n",
        "                model_2.train()  \n",
        "            else:\n",
        "                model_1.eval()\n",
        "                model_2.eval()          \n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                mb.child.comment = f'epoch {epoch+1} {stage}'\n",
        "                if stage == 'train':\n",
        "                    optimizer_1.zero_grad() # why is this not in original func?\n",
        "                    optimizer_2.zero_grad()\n",
        "                batch = tuple(t.to(args.device) for t in batch)\n",
        "                input_ids = batch[0]\n",
        "                attention_mask = batch[1]\n",
        "                token_type_ids = batch[2]\n",
        "                labels_1 = batch[3]\n",
        "                labels_2 = batch[4]\n",
        "\n",
        "                inputs_1 = {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": attention_mask,\n",
        "                    \"token_type_ids\": token_type_ids,\n",
        "                    \"labels\": labels_1\n",
        "                }\n",
        "\n",
        "                inputs_2 = {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": attention_mask,\n",
        "                    \"token_type_ids\": token_type_ids,\n",
        "                    \"labels\": labels_2\n",
        "                }\n",
        "                # model output: [loss, grad_fn (MSE), logits, batches]\n",
        "                outputs_1 = model_1(**inputs_1)\n",
        "                outputs_2 = model_2(**inputs_2)\n",
        "\n",
        "                loss_1 = outputs_1[0]\n",
        "                preds_1 = torch.round(outputs_1.logits).view(-1)\n",
        "                num_items += preds_1.shape[0]\n",
        "                num_correct_1 += (labels_1 == preds_1).sum()                \n",
        "                \n",
        "                loss_2 = outputs_2[0]                \n",
        "                preds_2 = torch.argmax(outputs_2.logits, dim=1)\n",
        "                num_correct_2 += (labels_2 == preds_2).sum()\n",
        "\n",
        "                # loss_1, loss_2 = loss_1 + loss_2, loss_1 + loss_2\n",
        "\n",
        "                if stage == 'train':\n",
        "                    loss_1.backward()\n",
        "                    loss_2.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model_1.parameters(),\n",
        "                                                   args.max_grad_norm)\n",
        "                    torch.nn.utils.clip_grad_norm_(model_2.parameters(),\n",
        "                                                   args.max_grad_norm)\n",
        "                    optimizer_1.step()\n",
        "                    optimizer_2.step()\n",
        "                    scheduler_1.step()\n",
        "                    scheduler_2.step()\n",
        "\n",
        "                    model_1.zero_grad()\n",
        "                    model_2.zero_grad()\n",
        "                    global_step += 1\n",
        "                running_loss += loss_1.item()\n",
        "                    \n",
        "\n",
        "            accuracies_1[stage] = num_correct_1 / num_items\n",
        "            accuracies_2[stage] = num_correct_2 / num_items\n",
        "            losses[stage] = running_loss / len(dataloader)\n",
        "            \n",
        "        # Save model checkpoint\n",
        "        if top_acc_1 < accuracies_1['val']:\n",
        "            if os.path.exists(args.output_dir_1):\n",
        "                shutil.rmtree(args.output_dir_1)\n",
        "            output_dir_1 = os.path.join(args.output_dir_1,\n",
        "                            \"1-checkpoint-{}\".format(epoch + 1))\n",
        "            if not os.path.exists(output_dir_1):\n",
        "                os.makedirs(output_dir_1)\n",
        "            model_1_to_save = (\n",
        "                model_1.module if hasattr(model_1, \"module\") else model_1\n",
        "            )    \n",
        "            model_1_to_save.save_pretrained(output_dir_1)\n",
        "            torch.save(args, os.path.join(args.output_dir_1, \"training_args.bin\"))\n",
        "            top_acc_1 = accuracies_1['val']\n",
        "\n",
        "        if top_acc_2 < accuracies_2['val']:\n",
        "            # Save model checkpoint\n",
        "            if os.path.exists(args.output_dir_2):\n",
        "                shutil.rmtree(args.output_dir_2)\n",
        "            output_dir_2 = os.path.join(args.output_dir_2,\n",
        "                                        \"2-checkpoint-{}\".format(epoch + 1))\n",
        "            if not os.path.exists(output_dir_2):\n",
        "                os.makedirs(output_dir_2)\n",
        "            model_2_to_save = (\n",
        "                model_2.module if hasattr(model_2, \"module\") else model_2\n",
        "            )\n",
        "            model_2_to_save.save_pretrained(output_dir_2)\n",
        "            torch.save(args, os.path.join(args.output_dir_2, \"training_args.bin\"))\n",
        "            top_acc_2 = accuracies_2['val']\n",
        "\n",
        "        print(accuracies_1, accuracies_2)\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            del model_1, model_2\n",
        "            torch.cuda.empty_cache()\n",
        "            break\n",
        "\n",
        "    return output_dir_1, output_dir_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "# tokenizer = args.tokenizer.from_pretrained(\n",
        "#     args.model_name_or_path,\n",
        "#     do_lower_case=args.do_lower_case\n",
        "# )\n",
        "\n",
        "train_dataset = load_dataset(args, tokenizer, mode=\"train\")\n",
        "val_dataset = load_dataset(args, tokenizer, mode=\"test\")\n",
        "datasets = {'train': train_dataset, 'val': val_dataset}\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "# torch.cuda.ipc_collect()\n",
        "# torch.cuda.memory_summary(abbreviated=True)\n",
        "\n",
        "\n",
        "# Train\n",
        "train(args, datasets)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/pretrained_model/koelectra_pretrained.bin were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/pretrained_model/koelectra_pretrained.bin and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-376c530cfd9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-376c530cfd9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, datasets)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mconfig_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     model_2 = args.model.from_pretrained(pretrain_bin,\n\u001b[0;32m--> 242\u001b[0;31m                                          config=config_2).to(args.device)\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m                 \u001b[0m_fast_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m             )\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0mold_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mnew_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mold_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFBaYi8S7h6",
        "outputId": "83d75bb8-139f-4588-fa95-929aac5427d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_model_build_end_time = printt(\"Model building: Start\")\n",
        "print(_model_build_end_time - _model_build_start_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10:14:12 Model building: Start\n",
            "312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZX6jC22S99-"
      },
      "source": [
        "#모델 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPEMmgXfTKf9",
        "outputId": "0965ffac-a2ea-4377-c617-5b591d87bca5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_start_time = printt(\"TEST: Start\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10:14:14 TEST: Start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLDvBcGMSdwg",
        "outputId": "812ab289-b030-4433-b58e-6bdd09db4f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#TODO: 해당 블럭에 테스트 수행을 위한 코드를 넣으세요. (시간측정 구간)\n",
        "#분석 파일은 tsv 파일로 제공되며, 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 =공백)으로 제공됩니다.\n",
        "\n",
        "from konlpy.tag import Kkma\n",
        "import re\n",
        "\n",
        "def test(args, test_data, mode, global_step=None):\n",
        "  \n",
        "    checkpoints_1 = list(\n",
        "                os.path.dirname(c) for c in\n",
        "                sorted(glob.glob(args.output_dir_1 + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "            )\n",
        "    checkpoints_2 = list(\n",
        "                os.path.dirname(c) for c in\n",
        "                sorted(glob.glob( args.output_dir_2 + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "            )\n",
        "    print(checkpoints_1, checkpoints_2)\n",
        "    checkpoint_1 = checkpoints_1[-1]\n",
        "    checkpoint_2 = checkpoints_2[-1]\n",
        "\n",
        "    model_1 = args.model.from_pretrained(checkpoint_1).to(args.device)\n",
        "    model_2 = args.model.from_pretrained(checkpoint_2).to(args.device)\n",
        "\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args.eval_batch_size)\n",
        "    num_correct_1, num_correct_2, num_items = 0, 0, 0\n",
        "    df_result_1 = pd.DataFrame(columns=['Review', 'Label1', 'Label2', 'Preds'])\n",
        "    df_result_2 = pd.DataFrame(columns=['Review', 'Label1', 'Label2', 'Preds'])\n",
        "    id_list = []\n",
        "    preds_1_list = []\n",
        "    preds_2_list = []\n",
        "    out_labels_1 = []\n",
        "    out_labels_2 = []\n",
        "    for batch in progress_bar(test_dataloader):\n",
        "        preds_temp = None\n",
        "        model_1.eval()\n",
        "        model_2.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }     \n",
        "            outputs_1 = model_1(**inputs)\n",
        "            outputs_2 = model_2(**inputs)\n",
        "            preds_1 = torch.round(outputs_1.logits).view(-1)\n",
        "            preds_2 = torch.argmax(outputs_2.logits, dim=1)\n",
        "            preds_1_list += list(preds_1.detach().cpu().numpy().astype(int))\n",
        "            preds_2_list += list(preds_2.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    return preds_1_list, preds_2_list\n",
        "\n",
        "def complete_result(list_preds):\n",
        "    label_dict_classification = {'중립': 0, '상담원': 1, '상담시스템': 2, '혜택': 3, '할부금융상품': 4,\n",
        "                  '카드상품': 5, '청구입금': 6, '심사/한도': 7, '생활편의서비스': 8,\n",
        "                  '상담/채널': 9, '리스렌탈상품': 10, '라이프서비스': 11, '금융상품': 12,\n",
        "                  '고객정보관리': 13, '삼성카드': 14, '기타': 15}\n",
        "    label_dict_sentiment = {'칭찬': 0, '불만': 1, '': -1}\n",
        "    label_dict_classification = dict((v, k) for k, v in label_dict_classification.items())\n",
        "    label_dict_sentiment = dict((v, k) for k, v in label_dict_sentiment.items())\n",
        "\n",
        "    for i in range(len(list_preds[1])):\n",
        "        if list_preds[1][i] == 0:\n",
        "            list_preds[0][i] = -1\n",
        "    for i in label_dict_classification:\n",
        "      if i not in [0, 1, 2, 15]:\n",
        "        label_dict_classification[i] = '>삼성카드>' + label_dict_classification[i]\n",
        "      else:\n",
        "        if i == 1 or i == 2:\n",
        "          label_dict_classification[i] = '>고객서비스>' + label_dict_classification[i]\n",
        "        elif i == 15:\n",
        "          label_dict_classification[i] = '>' + label_dict_classification[i]\n",
        "    sent_result = np.vectorize(label_dict_sentiment.get)(list_preds[0])\n",
        "    class_result = np.vectorize(label_dict_classification.get)(list_preds[1])\n",
        "    result_arr = np.core.defchararray.add(sent_result, class_result)\n",
        "    list_output = list(result_arr)\n",
        "    return list_output\n",
        "\n",
        "\n",
        "kkma = Kkma()\n",
        "# input_xlsx = '/content/drive/MyDrive/input_result.xlsx'\n",
        "xls = pd.ExcelFile('/content/drive/MyDrive/input_data.xlsx')\n",
        "test_data = pd.read_excel(xls, '학습데이터')\n",
        "df = pd.read_excel(xls, '학습데이터', usecols='C')\n",
        "df_temp = pd.DataFrame(columns=['Review'])\n",
        "for sentence in test_data['TEXT']:\n",
        "    sentence = re.sub('[^가-힣a-zA-Z0-9]', \"\", sentence)\n",
        "    review = '[SEP]'.join(kkma.sentences(sentence))\n",
        "    if [review] not in df.values[:, 0:1]:\n",
        "        df_temp = df.append(pd.DataFrame({'Review': review}, index=[id]))\n",
        "\n",
        "df_temp.to_csv('input_data.txt', header=None, index=None, sep='\\t', mode='a')\n",
        "with open('input_data.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        lines.append(line.strip())\n",
        "    lines_dict['input'] = lines\n",
        "input_dataset = load_dataset(args, tokenizer, mode=\"input\")\n",
        "\n",
        "list_preds = []\n",
        "preds_1, preds_2 = test(args, input_dataset, 'input')\n",
        "list_preds.append(preds_1)\n",
        "list_preds.append(preds_2)\n",
        "list_output = complete_result(list_preds)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['checkpoints_sentiment/1-checkpoint-1'] ['checkpoints_classification/2-checkpoint-1']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='251' class='' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [251/251 00:35<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLsLL-vSq1D",
        "outputId": "e93ae0fb-b682-43d0-e1bd-e23a06db7030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 수정금지: 타임스탬프\n",
        "_test_end_time = printt(\"Model building: Start\")\n",
        "print(_test_end_time - _test_start_time)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10:25:13 Model building: Start\n",
            "659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0fjGzq8a_0"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_NifWnpKw-"
      },
      "source": [
        "# 결과출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctiMXI8UQ0Q",
        "outputId": "e327f64b-b974-4354-be9d-1060dd41dc5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#TODO:해당 블럭에 테스트 결과를 파일로 저장하는 코드를 넣으세요. (시간측정 제외)\n",
        "#저장 파일은tsv 파일로 제공되는 학습데이터 파일과 동일한 레이아웃(단, 정답 분류 = 테스트 결과 도출된 양식)으로 저장\n",
        "\n",
        "import openpyxl\n",
        "myworkbook=openpyxl.load_workbook('/content/drive/MyDrive/input_data.xlsx')\n",
        "worksheet= myworkbook.get_sheet_by_name('학습데이터')\n",
        "for i in range(len(list_output)):\n",
        "    worksheet[f'D{i+2}']=list_output[i]\n",
        "worksheet['D1'] = 'INT'\n",
        "myworkbook.save(filename = '/content/drive/MyDrive/result_data.xlsx')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpdIIPmkMez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR-CGR8LIVQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uYU_MBjIYQr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}